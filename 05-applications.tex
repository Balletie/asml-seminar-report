\documentclass[multi,crop=false,class=article]{standalone}
\onlyifstandalone{\input{common}}

\begin{document}
\section{Applications}
\label{sec:applications}

As the previous chapters have shown, active state machine learning is a well
studied theoretical learning technology. Slowly but surely activate state
machine learning is moving from a theoretical novelty to a real world applicable
technology, due to hardware advancements as well as the optimizations discussed
in chapter \cref{sec:improvements}.

This chapter serves to highlight some interesting and important practical
applications of active state machine learning. The goal is not to discuss
individual applications, rather several ``domains'' of application have been
identified. Before doing so, however, the bridge between theory and application
must be crossed.

\subsection{From theory to practice}
\label{ssec:theory-to-practice}

The learning algorithms discussed in the previous chapters all required an
(abstract, formal) input alphabet, an (abstract, formal) output alphabet,
membership queries and equivalence queries. In practice, it is in many cases
quite difficult to realize these requirements for a specific system under
learning\cite{Stefen11a}. This section discusses these challenges faced when
applying active state machine learning to real world scenarios.

\paragraph{Interaction with real world applications} Before active state machine
learning can be used in practice, a mapping has to be made from an abstract,
formal input alphabet to a concrete real world ``language'' understood by
reactive systems (it is evident that reactive systems are a requirement for
practical active state machine learning)\todo{begrippenlijst aanmaken met
definities?}. This mapping needs to result in a deterministic language for it to
work\cite{Steffen11a}. In the same way, the output of the system under learning
needs to be mapped back to an abstract language understood by the tool (see
section \cref{sec:tools} below).

Whilst such a mapping has different peculiarities per application, there is an
overlapping theme that applies to every mapping: it needs to be abstract enough
in terms of communication (leading to a useful model structure, see below) while
allowing for an automatic back and forth translation between the abstract and
the concrete languages\cite{Steffen11a}. An interesting fact is that it turns
out that the active state machine learning algorithm can be enhanced per
application using application-specific optimizations\cite{Hungar03}. For more
information on recent work focusing on these abstractions, see
\cite{Aarts10,Howar11,Jonsson11}.

Besides mapping input- and output alphabets, the gap between the abstract
learned model and the concrete application also has to be bridged: when an
abstract learned model is presented to the user, it has to be ``translated'' to
a representation of the system under learning. This is rather intuitive for
those applications that are designed explicitly for connectivity (such as web
services or communication protocols), because these are made to be invoked from
the outside. For other types of applications, this can be arbitrarily difficult.

Another obstacle to overcome is that of parameters used in real world
applications. Think, for example, about increasing sequence numbers in
communication protocols. This is still a huge challenge to
resolve\cite{Steffen11a}, and, for now, besides the creation of prototypical
solutions\cite{Aarts10,Shahbaz07,Howar10}, application-specific solutions have
to be applied.\cite{Steffen11a}.

The final hurdle is that of a ``reset''. Membership queries have to be
independent (see chapter \cref{sec:noreset}). In practice this often means that
applications have to be reset in between successive membership queries. This can
be achieved using homing sequences\cite{Rivest93} (discussed in chapter
\cref{sec:variants} or by simply restarting the application for each membership
query.

\paragraph{Membership queries} Membership queries are the most straightforward
to translate to real world applications: they can be realized via testing. An
important thing to note, however, is the amount of membership queries required.
Learning a real world application can easily require several thousand membership
<<<<<<< 5f1b718e4942aa0e971de40c2538c90bcc9ab4ba
queries\todo{Add citation}. This means that the time required to learn a model
can be greatly reduced either by speeding up membership queries (executing them
in parallel), or simply by reducing the number of membership queries required.
Chapter \cref{sec:improvements} has discussed several improvements to achieve
exactly that. Additionally, application-specific optimizations can be used to
further reduce the number of membership queries required.

\paragraph{Equivalence queries} In theoretical simulations, equivalence testing
is often easy because often the target system (in some cases even the model!) is
known. In practice, however, the system under test is usually a black box
system. This means that equivalence queries will have to be approximated,
typically using membership queries. These approximated membership queries are in
general not decidable without assuming any extra knowledge\cite{Steffen11a},
such as the number of states of the system under learning: it is impossible to
be certain that the system has been tested extensively enough.

An alternative to using membership queries to simulate equivalence queries, is
to use model-based testing methods\cite{Broy05, Tretmans11}. An example of
model-based testing is Chow's W-method\cite{Chow78} (discussed in chapter
\cref{sec:improvements}), which can be used if an upper bound on the number of
states in the system is known.\todo{Mention Wp-method\cite{Fujiwara91} and more
examples? Or refer to chapter \cref{sec:variants}}

\todo{Discuss first paragraph of page 33 in \cite{Steffen11a}?}

All the above problems might give the impression that active state machine
learning is not yet applicable to real world applications. While it is true that
practical application is not yet complete, great progress has been made and
great things have already been achieved. The next few highlights of real world
applications serve to illustrate these facts.

\subsection{Tools}
\label{ssec:tools}

In order to encourage the use of active automata learning in a practical
setting, tools have to be created. This section will give a brief overview some
of the different tools that are available. Tools that are not considered are
are tools that are only used in automata verification, tools that are not
available like RALT \cite{Shahbaz:2014:ATB:2858086.2858089} and tools that are
not used a lot like AIDE \cite{Cicala2016}

\subsubsection{Learnlib}
\label{sssec:learnlib}

Learnlib  \footnote{In active development and available from
https://github.com/LearnLib } is a library that implements various active
learning algorithms as well as different configurations for learning
automata's. It is in development since 2009 \cite{Raffelt2009} and as of 2015,
there has been a
total overhaul of the tool \cite{Isberner2015}. To avoid confusion, the old
version was renamed
to JLearn.

\todo{Image of architecture of Learnlib }
The current version exists out of two parts: Automatalib and Learnlib.

\todo{Image of architecture of Automatalib }

\paragraph{Automatalib} An independent library that contains: an abstract
automata representations, automata data structures and algorithms. The abstract
automata representations makes the library flexible because all data structures
and algorithms depends on those representations. This makes it easy to add
third party like the BRICS library \cite{Alur:2005:SIS:1047659.1040314}. The
algorithms that are included are: minimalization and equivalence testing based
on Hopcroft and Karp's \cite{hopcroft1971linear} or the W-Method for black-box
testing.

\paragraph{Learnlib} A library that provides learning algorithms and
infrastructure for automata learning. The learning algorithms consists of a
'base algorithm' whereby the counterexample analysis can be exchanged with
other methods. All the different base algorithms with the examples of variants
that are officially supported are listed below:

\begin{itemize}
	\item L* (base)
	\begin{itemize}
		\item Maler \& Pnueli's
		\item Rivest \& Schapnire's
		\item Shahbaz's
		\item Suffix1by1
	\end{itemize}
	\item Obversation Pack (base)
	\item Kearns \& Vazirani's (base)
	\item DHC (base)
	\item TTT (base)
	\item NL*
\end{itemize}

All the algorithms come with both DFA and Mealy versions, expect for DHC and NL*.

For finding the counterexamples, Learnlib uses Automatalib as well as other
methods like randomized tests. More methods can be found in \cite{Isberner2015}.

Learlib also offers filters for reducing the amount of queries such as
elimination of duplicate queries. %Is hier een theoriestuk over?
It also contains a parallellization component that can speed up the process by
using multiple teachers and parallel execution of membership
queries\cite{Henrix15}\cite{Howar2012}.

\subsubsection{Libalf}
\label{sssec:libalf}
Libalf\footnote{Not in active development since 2011 but still available
from http://libalf.informatik.rwth-aachen.de} is a library for learning and
manipulating formal languages. It has both active and passive learning
algorithms. For this paper, only the active part is covered.

\todo{Image of architecture of libalf }
The library consists of a core library as well as two additional libraries:liblangen(random regular language generator) and AMoRE++(
automata library that included DFA,NFA,Mealy and is extendable).

The core consists of the learning algorithms and the knowledgebase. The latter
stores language information and collects the different queries that are used
for that language. This storage makes it possible to switch learning algorithms
or use multiple learning algorithms during the learning process. The active
learning algorithms that it offers are listed below.

\begin{itemize}
	\item L*
	\item Rivest \& Schapnire's
	\item NL*
	\item Kearns \& Vazirani's
\end{itemize}

Besides the algorithms and the knowledgebase, it has filters for reducing the
amount of queries asks to the teacher. These filters uses domain specific
knowledge. %Is hier theorie over?
Also, it provides methods that uses domain specific relation for reducing the
amount of storage needed.

More information on Libalf is published in \cite{Bollig2010}.

\subsubsection{Tomte}
\label{sssec:tomte}
Tomte \footnote{In active development and available from
http://tomte.cs.ru.nl/Tomte-0-4} is a tool that automatically makes
abstractions for automata learning. Essentially, it a connector between the
system under learning(SUL) and the learner. This makes using Learnlib and
Libalf easier, since the user doesn't have to make the mapping.

\todo{Image of architecture of Tomte }

The Abstractor, Lookahead Oracle and the Determinizer together forms Tomte. The
other two parts are not considered part
of Tomte but it comes with a supplied library (Learnlib) for making the learner.
The makers also have a tool \footnote{SUL Tool available from
http://tomte.cs.ru.nl/Sut-0-4/Description} available for creating the SUL since
they must be modeled after a register automata.

\paragraph{Determinizer}
This parts elimates the nondeterministic behaviour caused by the SUL. Since
tools like Learnlib can only analyse deterministic behaviour, it needs to
converted. The theory behind it, is explained in \cite{Aarts2015}.

\paragraph{Lookahead Oracle}
This oracle is used to annotate each output action of the SUL with values that
has an impact on the future behaviour of the SUL. This makes it is possible to
learn any deterministic register automaton. The theory and implementation of
this oracle is found in \cite{Aarts2014} and \cite{tomte14}.

\paragraph{Abstractor}
The Abstractor is the component that creates the mapping between the SUL and
the learner. The idea behind the mapper is to make an abstraction of the
parameter values of the SUL but leaving the input/output symbols unchanged. It
uses counterexample-guided abstraction refinement\cite{tomte14} for extending
of the mapping. In order to make it scalable, this component also tries to
reduce the length of the counterexample by removing loops and single
transistion \cite{Koopman2014}. The complete theory is found in \cite{tomte14}.

\subsubsection{Performance comparisons}
\label{ssec:performance_compare}
This section gives an overview on what the perfomance is of each the tools
listed. It uses the comparisons that are published in
\cite{Aarts2014}\cite{Aarts2015} and
the website of
Learnlib.

\paragraph{Learnlib vs JLearn}

\todo{Image of comparison between Learnlib and JLearn] }
%language of size ten and the automaton to be learned has 500 states
%(learnlib.de/features/jlearn-performance-comparison)
This comparion is between the old version of Learnlib (JLearn) and the new
version. For this comparisons, the tools have to learn a randomly generated
alphabet of a certain size which are implemented in Mealy machine of various
state sizes.

X shows that for this comparison, the new version of Learnlib is
significant faster with all algorithms than JLearn.

\paragraph{Learnlib vs Libalf}
\todo{Image of comparison between Learnlib and Liblaf }
%(learnlib.de/features/libalf-performance-comparison)
The second comparison is between the new version of Learnlib and Libalf.
The same setup is used as the previous one, however the tools have to learn a
DFA instead.

This comparison shows that Learnlib performs a lot better than Libalf.

\paragraph{Tomte vs Learnlib}
\todo{Image of comparison between Learnlib and Tomte }
The last comparison is between Tomte(with learnlib) and Learnlib alone.
For this setup, a number of real world models are used. These includes: the
Biometric Passport \cite{Aarts2010}, a login procedure and the session
initiation protocol. The tools had to learn a register automata. This meant
that support for learning register automata for Learnlib to be added.
\footnote{Source code and publications can be found here
https://github.com/LearnLib/raxml}

According to X, Tomte performs better than Learnlib and can learn more models
than Learnlib. However, Learnlib can outperform Tomte if during testing a
perfect equivalence oracle is available. This is shown and explained in
\cite{Aarts2014}.


In conclusion, from the comparisons it is clear that there is a lot of
improvement in the creation of the tools in recent years by using the new
methods that are found by either testing the tools on practical applications or
new advances in active automata learning.

\subsection{Protocol State Fuzzing}
One commonly used application of active state machine learning is
\texttt{protocol state fuzzing}\cite{deRuiter15,Aarts13,Cho10,Aarts10}. This
boils down to checking the correctness of some implementation of a protocol, by
using the protocol description to identify states and allowed transitions. The
input alphabet is defined as a set of messages. Combinations of these messages
are then constructed and passed as input. If the actual output matches the 
expected output, the logic of the implemtation is correct. However, if the 
expected output does not match the actual output, there is some logical error
in the implementation of the protocol. The tests are performed as black box
testing, resulting in the need for approximation as explained in
\ref{sec:theory-to-practice}.

To illustrate, consider an implementation of the TLS-protocol\cite{deRuiter15}.
For this specific protocol fuzzing application, an improved version of the
W-method\cite{Chow78} is used. The improvement comes forth from the fact that
the TLS-protocol requires the output to be ``Connection closed'' at all times
once a connection is closed. By making use of this and thus limiting the
W-method to halt whenever the connection is closed, a great reduction in the
time needed to perfom the implementation analysis is achieved. Furhtermore, 
especially since the W-method is a very costly one, interrupting the trace 
generation as soon as the connection has been closed reduces the number of 
equivalence queries significantly.
An aspect that has proven to be difficult in practice, namely that of resetting
the state of the machine after each iteration, has been overcome by De Ruiter
et al. They came up with an abstraction of the input and output alphabet, in
order to be able to define a ``reset''-state, as well as to generalize states 
between different TLS-implementations. The reset command trigges actions to
not only reset the state, but also to undo any possible changes to variables.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
