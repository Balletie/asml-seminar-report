\documentclass[multi,crop=false,class=article]{standalone}
\onlyifstandalone{\input{common}}

\begin{document}
\section{Applications}
\label{sec:applications}

As the previous chapters have shown, active state machine learning is a well
studied theoretical learning technology. Slowly but surely activate state
machine learning is moving from a theoretical novelty to a real world applicable
technology, due to hardware advancements as well as the optimizations discussed
in chapter \cref{sec:improvements}.

This chapter serves to highlight some interesting and important practical
applications of active state machine learning. The goal is not to discuss
individual applications, rather several ``domains'' of application have been
identified. Before doing so, however, the bridge between theory and application
must be crossed.

\subsection{From theory to practice}
\label{ssec:theory-to-practice}

The learning algorithms discussed in the previous chapters all required an
(abstract, formal) input alphabet, an (abstract, formal) output alphabet,
membership queries and equivalence queries. In practice, it is in many cases
quite difficult to realize these requirements for a specific system under
learning\cite{Stefen11a}. This section discusses these challenges faced when
applying active state machine learning to real world scenarios.

\paragraph{Interaction with real world applications} Before active state machine
learning can be used in practice, a mapping has to be made from an abstract,
formal input alphabet to a concrete real world ``language'' understood by
reactive systems (it is evident that reactive systems are a requirement for
practical active state machine learning)\todo{begrippenlijst aanmaken met
definities?}. This mapping needs to result in a deterministic language for it to
work\cite{Steffen11a}. In the same way, the output of the system under learning
needs to be mapped back to an abstract language understood by the tool (see
section \cref{sec:tools} below).

Whilst such a mapping has different peculiarities per application, there is an
overlapping theme that applies to every mapping: it needs to be abstract enough
in terms of communication (leading to a useful model structure, see below) while
allowing for an automatic back and forth translation between the abstract and
the concrete languages\cite{Steffen11a}. An interesting fact is that it turns
out that the active state machine learning algorithm can be enhanced per
application using application-specific optimizations\cite{Hungar03}. For more
information on recent work focusing on these abstractions, see
\cite{Aarts10,Howar11,Jonsson11}.

Besides mapping input- and output alphabets, the gap between the abstract
learned model and the concrete application also has to be bridged: when an
abstract learned model is presented to the user, it has to be ``translated'' to
a representation of the system under learning. This is rather intuitive for
those applications that are designed explicitly for connectivity (such as web
services or communication protocols), because these are made to be invoked from
the outside. For other types of applications, this can be arbitrarily difficult.

Another obstacle to overcome is that of parameters used in real world
applications. Think, for example, about increasing sequence numbers in
communication protocols. This is still a huge challenge to
resolve\cite{Steffen11a}, and, for now, besides the creation of prototypical
solutions\cite{Aarts10,Shahbaz07,Howar10}, application-specific solutions have
to be applied.\cite{Steffen11a}.

The final hurdle is that of a ``reset''. Membership queries have to be
independent (see chapter \cref{sec:noreset}). In practice this often means that
applications have to be reset in between successive membership queries. This can
be achieved using homing sequences\cite{Rivest93} (discussed in chapter
\cref{sec:variants} or by simply restarting the application for each membership
query.

\paragraph{Membership queries} Membership queries are the most straightforward
to translate to real world applications: they can be realized via testing. An
important thing to note, however, is the amount of membership queries required.
Learning a real world application can easily require several thousand membership
queries\todo{Add citation}. This means that the time required to learn a model
can be greatly reduced either by speeding up membership queries (executing them
in parallel), or simply by reducing the number of membership queries required.
Chapter \cref{sec:improvements} has discussed several improvements to achieve
exactly that. Additionally, application-specific optimizations can be used to
further reduce the number of membership queries required.

\paragraph{Equivalence queries} In theoretical simulations, equivalence testing
is often easy because often the target system (in some cases even the model!) is
known. In practice, however, the system under test is usually a black box
system. This means that equivalence queries will have to be approximated,
typically using membership queries. These approximated membership queries are in
general not decidable without assuming any extra knowledge\cite{Steffen11a},
such as the number of states of the system under learning: it is impossible to
be certain that the system has been tested extensively enough.

An alternative to using membership queries to simulate equivalence queries, is
to use model-based testing methods\cite{Broy05, Tretmans11}. An example of
model-based testing is Chow's W-method\cite{Chow78} (discussed in chapter
\cref{sec:improvements}), which can be used if an upper bound on the number of
states in the system is known.\todo{Mention Wp-method\cite{Fujiwara91} and more
examples? Or refer to chapter \cref{sec:variants}}

\todo{Discuss first paragraph of page 33 in \cite{Steffen11a}?}

All the above problems might give the impression that active state machine
learning is not yet applicable to real world applications. While it is true that
practical application is not yet complete, great progress has been made and
great things have already been achieved. The next few highlights of real world
applications serve to illustrate these facts.

\subsection{Protocol State Fuzzing}
One commonly used application of active state machine learning is
\texttt{protocol state fuzzing}\cite{deRuiter15,Aarts13,Cho10,Aarts10}. This
boils down to checking the correctness of some implementation of a protocol, by
using the protocol description to identify states and allowed transitions. The
input alphabet is defined as a set of messages. Combinations of these messages
are then constructed and passed as input. If the actual output matches the 
expected output, the logic of the implemtation is correct. However, if the 
expected output does not match the actual output, there is some logical error
in the implementation of the protocol. The tests are performed as black box
testing, resulting in the need for approximation as explained in
\ref{sec:theory-to-practice}.

To illustrate, consider an implementation of the TLS-protocol\cite{deRuiter15}.
For this specific protocol fuzzing application, an improved version of the 
W-method\cite{Chow78} is used. The improvement comes forth from the fact that 
the TLS-protocol requires the output to be ``Connection closed'' at all times
once a connection is closed. By making use of this and thus limiting the
W-method to halt whenever the connection is closed, a great reduction in the
time needed to perfom the implementation analysis is achieved. Furhtermore, 
especially since the W-method is a very costly one, interrupting the trace 
generation as soon as the connection has been closed reduces the number of 
equivalence queries significantly.
An aspect that has proven to be difficult in practice, namely that of resetting
the state of the machine after each iteration, has been overcome by De Ruiter
et al. They came up with an abstraction of the input and output alphabet, in
order to be able to define a ``reset''-state, as well as to generalize states 
between different TLS-implementations. The reset command trigges actions to
not only reset the state, but also to undo any possible changes to variables.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
