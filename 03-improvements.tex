\documentclass[multi,crop=false,class=article]{standalone}
\onlyifstandalone{\input{common}}

\begin{document}
\section{Improvements}
\label{sec:improvements}
Since Angluin's original paper, several improvements have been made with respect
to the original $L^*$ algorithm described in \cref{sec:an-algor-learn}. The
first improvement that will be discussed is the classification tree. This
improvement forms the basis for multiple subsequent improvements (including the
TTT-algorithm).

In discussing the improvements, two kinds of comparisons are made between them
and the original $L^*$ algorithm. The first comparison is the difference in
query complexity, the second is in terms of the space complexity.

\subsection{Classification Trees}
\label{sec:classification-trees}
Introduced by Kearns and Vazirani\cite{Kearns1994}, classification trees (or
\textit{discrimination trees}) are meant as a replacement for the observation
table (see \cref{sec:data-structure}). In other words, it is a different data
structure for maintaining the necessary information to form a hypothesis
automaton.

The structure of a classification tree is as follows: all internal nodes of the
tree are labeled by a distinguishing extension $e$, and the leaf nodes are
labeled by an access string $s$. For each internal node labeled $e$, the leaf
nodes are in its right or left subtree depending on whether
$\mq(s\concat e)$ equals ``yes'' or ``no''. The root of the tree is labeled
with $\epsilon$.

From the way the tree is constructed, any pair of access strings $v,w \in S$ are
distinguished by their lowest common ancestor in the tree. It follows that,
since any pair is distinguished by a distinguishing extension, any pair of
access strings in the classification tree are Nerode-inequivalent (see
\cref{def:nerode-eq}). Therefore each access string $s \in S$ uniquely
identifies an equivalence class $\eqclass{s}{\lang_M}$ of the target automaton
$M$.

An essential concept is the \textit{sifting} operation (see \cref{alg:sift}),
which for any string $w$ finds the identifying access string of
$\eqclass{w}{\lang_M}$. The operation is denoted as $\sift(w)$ and works as
follows: starting at the root node, for each internal node labeled with $e$, go
left or right in the tree depending on $\mq(s \concat e)$. This is done until a
leaf node has been reached, which is labelled with the correct access
string. The sifting operation can be seen as the replacement for the $\row(w)$
operation for observation tables, to identify equivalence classes (and thus
states) in the hypothesis. The construction of the hypothesis $\hat M$ is thus
done in the same way, using the sifting operation. Note that all the leaf nodes
that are in the right subtree of the root node define the accepting states of
$\hat M$.

\begin{algorithm}[b]
  \caption{Find the identifying access string of the equivalence class of a
    given word $w \in \Sigma^*$.}
  \label{alg:sift}
  \begin{algorithmic}[1]
    \Function{Sift}{n,w} \Comment{In the first call, $n$ is equal to the root
      node labeled $\epsilon$.}
      \If{$n$ is a leaf node}
        \State\Return $n.\text{label}$
        \Comment{$n.\text{label}$ is the access string that identifies   $\eqclass{w}{\lang_M}$.}
      \EndIf
      \If{$\mq(w \concat n.\text{label})$}
        \Comment{$n.\text{label}$ is a distinguishing extension.}
        \State $\sift(n.\text{right},w)$
      \Else
        \State $\sift(n.\text{left},w)$
      \EndIf
    \EndFunction{}
  \end{algorithmic}
\end{algorithm}

Howar~et~al. state that a classification tree is redundancy-free because each
state in $\hat M$ is distinguished by one distinguishing extension, as opposed
to observation tables where states are distinguished by a fixed amount of
distinguishing extensions\cite{Howar2014}.

\subsubsection{Complexity analysis}
\label{sec:complexity-analysis}
With their introduction of the classification tree, Kearns and Vazirani also
adapted the original $L^*$ learning algorithm to use the tree
structure\cite{Kearns1994}. The precise operation of the adapted algorithm will
not be given, but we do give an overview of the impact on the query complexity
and the space complexity.

\paragraph{Query Complexity} The amount of membership queries for hypothesis
construction is bounded by $\mathcal{O}(kn^2)$ membership queries in the
worst-case (i.e. a degenerated
tree)\cite{Howar2014,Kearns1994,Isberner2014b}. For the counterexample analysis
the amount of queries is bounded by $\mathcal{O}(nm)$\cite{Kearns1994}. Thus in
total the query complexity is bounded by $\mathcal{O}(kn^2 + nm)$, an
improvement over that of $L^*$.

\paragraph{Space Complexity} In chapter 4 of his dissertation, Isberner analyzed
the space complexity of classification tree based learners as follows: a
degenerated tree has $\mathcal{O}(n)$ amount of nodes, of which each inner node
is labeled by a distinguishing extension of length
$\mathcal{O}(m)$\cite{Isberner2015a}.  The space required for the hypothesis
automaton is $\mathcal{O}(kn)$\cite{Isberner2015a}. Thus the algorithm requires
space in $\mathcal{O}(kn + mn)$\cite{Isberner2014b,Isberner2015a}, which is an
improvement on the space complexity of the observation table of $L^*$.

\subsection{Rivest and Schapire's Counterexample Analysis}
\label{sec:rivest-schap-count}
Rivest and Schapire made a significant improvement\cite{Rivest1993} to the
\textit{counterexample analysis} stage, which is the part that handles
counterexamples returned by an equivalence query.

When a counterexample $\gamma$ is obtained, Rivest and Schapire's improved
algorithm finds a \textit{single} extension $e$ from $\gamma$ that distinguishes
two states in the new hypothesis and adds it to the set $E$ of distinguishing
extensions. This is in contrast to the original $L^*$ algorithm, which adds all
prefixes of the counterexample to the set $S$ of access strings.

According to Rivest and Schapire\cite{Rivest1993} and further clarified by
Steffen\cite{Steffen2011} and Isberner\cite{Isberner2014a}, there must be a
point where the hypothesis automaton $\hat M$ takes a wrong turn (i.e. there is
an incorrect transition). Let $e$ be the suffix of the counterexample $\gamma$
from the point where the transition of $\hat M$ is inconsistent with the
corresponding transition of the target $M$. Then $e$ can be found by means of
membership queries and a search strategy such as binary search.

\paragraph{Query complexity} Rivest and Schapire's original improvement used a
binary search strategy to find the single suffix $e$. As expected from a binary
search, this results in $\mathcal{O}(\log(m))$ membership queries per
counterexample of length $m$. The amount of equivalence queries is
$\mathcal{O}(n)$, thus the amount of membership queries due to counterexample
analysis is $\mathcal{O}(n\log(m))$. The total amount of membership queries
including hypothesis construction is thus $\mathcal{O}(kn^2 + n\log(m))$, an
improvement over that of Kearns and Vazarani's (see
\cref{sec:complexity-analysis}).

It is worthwhile to note that Isberner~et~al. analyzed and compared multiple
search strategies by means of an abstract framework\cite{Isberner2014a}. Their
results show that the query complexities of the other search strategies remain
the same as the original binary search strategy due to Rivest and Schapire, yet
perform better when put into practice.

\paragraph{Space Complexity} From Isberner's analysis\cite{Isberner2014b}, the
space complexity due to the new counterexample analysis is better than that of
$L^*$, however it is worse than that of Kearns and Vazirani's:
$\mathcal{O}(kn^2 + nm)$.

In the next section we will show another version of the algorithm due to
Isberner~et~al., which combines Kearns and Vazirani's classification tree, and
Rivest and Schapire-style counterexample analysis.  \todo{actually write this
  section?}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
