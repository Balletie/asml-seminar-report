\documentclass[multi,crop=false,class=article]{standalone}
\onlyifstandalone{\input{common}}

\usepackage{common}

\begin{document}
\section{Improvements}
\label{sec:improvements}
There are several improvements to be made with respect to the original $L^*$
algorithm described in \cref{sec:fundamental-theory}, which will be discussed in
this chapter. In discussing the improvements, we make two kinds of comparisons
between the algorithms. The first comparison is in terms of the amount of
\textit{queries} (the \textit{query complexity}), which gives an indication of
the running time of the algorithm.

In the second comparison, we show the improvements in terms of \textit{space
  complexity} between algorithms. We discuss the TTT-algorithm, which according
to Isberner~et~al. exhibits an optimal space complexity\cite{Isberner14b}.

The first improvement which will be discussed is the \textit{classification
  tree}, which forms the basis for multiple subsequent improvements (including
the TTT-algorithm).

\subsection{Classification Trees}
\label{sec:classification-trees}
Introduced by Kearns and Vazirani\cite{Kearns94}, classification trees (or
\textit{discrimination trees}) are meant as a replacement for the observation
table (see \cref{sec:data-structure}). In other words, it is a different data
structure for storing the sets $S$ and $E$ of access strings and distinguishing
extensions. The main characteristic of a classification tree is that it is
\textit{redundancy-free}, which will be explained in this section.

Say that we have a classification tree for some target automaton $M$. The labels
of the internal nodes of the tree are distinguishing extensions from the set
$E$, and the labels of leaf nodes are access strings from $S$. The tree's
structure is such that for any internal node $e \in E$, its left subtree
contains all $s \in S$ such that $s \concat e$ is rejected by $M$, and its right
subtree contains those $s$ such that $s \concat e$ is accepted by $M$.

From the way the tree is constructed, any pair of access strings $v,w \in S$ are
distinguished by their \textit{lowest common ancestor} in the tree. Thus, $v$
and $w$ are distinguished by exactly \textit{one} distinguishing extension. In
other words, any pair of access strings in the classification tree are
Nirode-inequivalent~\todo{reference subsection chpt 2}, and thus each access
string uniquely identifies a distinct equivalence class of the target automaton
$M$. When we construct a hypothesis, each equivalence class (and thus each
access string), corresponds to a state in our hypothesis automaton $\hat M$.

Howar~et~al. say that a discrimination tree is \textit{redundancy-free} because
each state in $\hat M$ is distinguished by one distinguishing extension, as
opposed to observation tables where states are distinguished by a fixed amount
of distinguishing extensions\cite{Howar14}.

\subsubsection{Complexity analysis}
\label{sec:complexity-analysis}
With their introduction of the \textit{classification tree}, Kearns and Vazirani
also adapted the original $L^*$ learning algorithm to use the tree
structure\cite{Kearns94}. The precise working of the adapted algorithm will not
be given, but we do give an overview of the impact on the query complexity and
the space complexity.

\paragraph{Query Complexity} The amount of \textit{membership queries} for
hypothesis construction is bounded by $\mathcal{O}(kn^2)$ membership queries in
the worst-case (i.e. a degenerated tree)\cite{Howar14,Kearns94,Isberner14b},
where $k = |\Sigma |$ and $n$ is the size of the target automaton $M$. For the
counterexample analysis the amount of queries is bounded by $\mathcal{O}(nm)$,
where $m$ is the size of the longest counterexample\cite{Kearns94}. Thus in
total the query complexity is bounded by $\mathcal{O}(kn^2 + nm)$, an
improvement over that of $L^*$.

\paragraph{Space Complexity} In chapter 4 of his dissertation, Isberner analyzed
the space complexity of discrimination-tree based learners as follows: a
degenerated tree has $\mathcal{O}(n)$ amount of nodes, of which each inner node
is labeled by a distinguishing extension of length
$\mathcal{O}(m)$\cite{Isberner15}.  The space required for the hypothesis
automaton is $\mathcal{O}(kn)$\cite{Isberner15}. Thus the algorithm requires
space in $\mathcal{O}(kn + mn)$\cite{Isberner14b,Isberner15}\todo{Compare with
  L-star}.

\subsection{Rivest and Schapire's Counterexample Analysis}
\label{sec:rivest-schap-count}
In 1993, Rivest and Schapire published a paper on learning without a reset using
homing sequences\cite{Rivest93}, which will be the scope of discussion of
\cref{sec:variants}\todo{Change reference to the No-Reset variant specifically}.

Their paper, however, also included an improvement upon $L^*$, also in the case
that a reset \textit{is} available. Specifically, they improved the
\textit{counterexample analysis} stage, which is the part that handles
counterexamples returned by an equivalence query.

When a counterexample $\gamma$ is obtained, Rivest and Schapire's improved
algorithm finds a \textit{single} extension $e$ from $\gamma$ distinguishing two
states in the new hypothesis, and adds it to the set $E$ of distinguishing
extensions. This is in contrast with the original $L^*$ algorithm, which adds
all prefixes of the counterexample to the set $S$ of access strings.

According to Rivest and Schapire\cite{Rivest93}, and further clarified by
Steffen\cite{Steffen11} and Isberner\cite{Isberner14a}, there must be a point
where the hypothesis automaton $\hat M$ takes a wrong turn (i.e. there's an
incorrect transition). Let $e$ be the suffix of the counterexample $\gamma$ from
the point where the transition of $\hat M$ is inconsistent with the
corresponding transition of the target $M$. Then $e$ can be found by means of a
search strategy such as binary search and membership queries.

As in the previous section, this section concludes with an analysis of the
impact on the amount of membership queries needed and the space requirement.

\paragraph{Query complexity} Rivest and Schapire's original improvement used a
binary search strategy to find the single suffix $e$. As expected from a binary
search, this results in $\mathcal{O}(\log(m))$ membership queries per
counterexample of length $m$. The amount of equivalence queries is
$\mathcal{O}(n)$, and thus the amount of membership queries due to
counterexample analysis is $\mathcal{O}(n\log(m))$, where $m$ is the size of the
longest counterexample. The total amount of membership queries including
hypothesis construction is thus $\mathcal{O}(kn^2 + n\log(m))$, an improvement
over that of Kearns and Vazarani's (see \cref{sec:complexity-analysis}).

It is worthwhile to note that Isberner~et~al. analyzed and compared multiple
search strategies by means of an abstract framework\cite{Isberner14a}. Their
results show that the query complexities of the other search strategy remain the
same as the original binary search strategy due to Rivest and Schapire, yet
perform better when put into practice.

\paragraph{Space Complexity} From Isberner's analysis\cite{Isberner14b}, the
space complexity due to the new counterexample analysis is better than that of
$L^*$, however it is worse than that of Kearns and Vazirani's:
$\mathcal{O}(kn^2 + nm)$.

In the next section we will show another version of the algorithm due to
Isberner~et~al., which combines Kearns and Vazirani's discrimination tree, and
Rivest and Schapire-style counterexample analysis.

\subsection{Chow's W-method}
Part of the active state machine learning is the equivalence check.
Since the actual state machine is not known we need to approximate it by
utilizing model-based testing.
This can be done by using Chow's W-method \cite{deRuiter15, Chow78}.

According to \cite{vasilevskii73}, the maximal number of test sequences is 
$n^{2} \times k^{m-n+1}$ and the maximal length is 
$n^{2} \times m \times k^{m-n+1}$.
In which n is the number of states in the provided automaton, m is the number
of states in the correct automaton, and k is the size of the input alphabet.

By refining the test suites and the specification at the same time, and
using a divide and conquer approach the size of the test is
considerably reduced in comparison to defining the tests from
the final specification\cite{Ipate07}.
This is especially beneficial because complex systems are usually
created in multiple iterations.
Unlike some other test selection methods the W-method does not require
that the number of states is exactly the same between
the implementation and protocol.
The W-method generates tests that guarantee the correct the
correctness of the implementation with a number of states below a
certain upper bound.

The W-method generates input sequences that reach every state in the
diagram, check all the transitions in the diagram and identify all the
destination states and verify them against their counterparts in
the implementation\cite{Ipate07}.
The W-method constructs two sets of input sequences.
A state cover $S \subseteq \Sigma^{*}$ that reaches every state in the
finale state machine.
The empty sequence is also included to reach the initial state.
And a characterization set $W \subseteq \Sigma^{*}$ that has different
outputs for at least one sequence in $W$ for every pair of different states.

We have a specification P that can be modelled by an unknown finite
state machine M.
The only information we need is an estimate of the maximum number of
states of an unknown model for the machine.
Suppose $d = \text{estimated maximum number of states for P}
- \text{number of states in M}$.
Naturally, it follows that $d \geq 0$.
We also take $\Sigma \left[ n \right] = \Sigma^{n} \cup \dots \cup \lbrace \epsilon \rbrace$,
making $\Sigma\left[ n \right]$ a random word with a maximal length of n.
For the W-method the testing suite $T = S \concat \Sigma\left[ d + 1 \right]  \concat W$.
The idea behind the W-method is that $S \concat \Sigma\left[ 1 \right]  = S \cup S
\concat \Sigma$, which is also called the transition cover of $M$, ensures that all the
states and transition in $P$ are also present in $M$, while
$\Sigma\left[ n \right] \concat W$ ensures that $M$ and $P$ are in the same state after
performing all the transitions.

In order to use the W-method there are four requirements.
The machine must be minimal, completely specified, completely reachable, 
and have a fixed initial state.
With some additional manipulation these four assumptions may be violated.
The method consists of three steps.
\begin{enumerate}
\item estimation of the maximum number of states.
\item generate test sequences.
\item verify the responses.
\end{enumerate}

In some fields the W-method can be adapted to reduce the number of test
cases even further such as for server testing.
When a server closes the connection there usually are no further
transitions possible.
This can greatly reduce the number of tests necessary\cite{deRuiter15}.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
