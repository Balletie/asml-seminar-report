\documentclass[multi,crop=false,class=article]{standalone}
\onlyifstandalone{\input{common}}

\usepackage{common}

\begin{document}
\section{Improvements}
\label{sec:improvements}
There are several improvements to be made with respect to the original $L^*$
algorithm described in \cref{sec:fundamental-theory}, which will be discussed in
this chapter. In discussing the improvements, we make two kinds of comparisons
between the algorithms. The first comparison is in terms of the amount of
\textit{queries} (the \textit{query complexity}), which gives an indication of
the running time of the algorithm.

In the second comparison, we show the improvements in terms of \textit{space
  complexity} between algorithms. We discuss the TTT-algorithm, which according
to Isberner~et~al. exhibits an optimal space complexity\cite{Isberner14b}.

The first improvement which will be discussed is the \textit{classification
  tree}, which forms the basis for multiple subsequent improvements (including
the TTT-algorithm).

\subsection{Classification Trees}
\label{sec:classification-trees}
Introduced by Kearns and Vazirani\cite{Kearns94}, classification trees (or
\textit{discrimination trees}) are meant as a replacement for the observation
table (see \cref{sec:data-structure}). In other words, it is a different data
structure for storing the sets $S$ and $E$ of access strings and distinguishing
extensions. The main characteristic of a classification tree is that it is
\textit{redundancy-free}, which will be explained in this section.

Say that we have a classification tree for some target automaton $M$. The labels
of the internal nodes of the tree are distinguishing extensions from the set
$E$, and the labels of leaf nodes are access strings from $S$. The tree's
structure is such that for any internal node $e \in E$, its left subtree
contains all $s \in S$ such that $s \concat e$ is rejected by $M$, and its right
subtree contains those $s$ such that $s \concat e$ is accepted by $M$.

From the way the tree is constructed, any pair of access strings $v,w \in S$ are
distinguished by their \textit{lowest common ancestor} in the tree. Thus, $v$
and $w$ are distinguished by exactly \textit{one} distinguishing extension. In
other words, any pair of access strings in the classification tree are
Nirode-inequivalent~\todo{reference subsection chpt 2}, and thus each access
string uniquely identifies a distinct equivalence class of the target automaton
$M$. When we construct a hypothesis, each equivalence class (and thus each
access string), corresponds to a state in our hypothesis automaton $\hat M$.

Howar~et~al. say that a discrimination tree is \textit{redundancy-free} because
each state in $\hat M$ is distinguished by one distinguishing extension, as
opposed to observation tables where states are distinguished by a fixed amount
of distinguishing extensions\cite{Howar14}.

\subsubsection{Complexity analysis}
\label{sec:complexity-analysis}
With their introduction of the \textit{classification tree}, Kearns and Vazirani
also adapted the original $L^*$ learning algorithm to use the tree
structure\cite{Kearns94}. The precise working of the adapted algorithm will not
be given, but we do give an overview of the impact on the query complexity and
the space complexity.

\begin{description}
\item[Query Complexity] The amount of \textit{membership queries} for hypothesis
  construction is bounded by $\mathcal{O}(kn^2)$ membership queries in the
  worst-case (i.e. a degenerated tree)\cite{Howar14,Kearns94,Isberner14b}, where
  $k = |\Sigma |$ and $n$ is the size of the target automaton $M$. For the
  counterexample analysis the amount of queries is bounded by $\mathcal{O}(nm)$,
  where $m$ is the size of the longest counterexample\cite{Kearns94}.

  Thus in total the query complexity is bounded by $\mathcal{O}(kn^2 + nm)$, an
  improvement over that of $L^*$.
\item[Space Complexity] In chapter 4 of his dissertation, Isberner analyzed the
  space complexity of discrimination-tree based learners as follows: a
  degenerated tree has $\mathcal{O}(n)$ amount of nodes, of which each inner
  node is labeled by a distinguishing extension of length
  $\mathcal{O}(m)$\cite{Isberner15}.  The space required for the hypothesis
  automaton is $\mathcal{O}(kn)$\cite{Isberner15}. Thus the algorithm requires
  space in $\mathcal{O}(kn + mn)$\cite{Isberner14b,Isberner15}\todo{Compare with
    L-star}.
\end{description}

\subsection{Chow's W-method}
Part of the active state machine learning is the equivalence check.
Since the actual state machine is not known we need to approximate it by
utilizing model-based testing.
This can be done by using Chow's W-method \cite{deRuiter15, Chow78}.

According to \cite{vasilevskii73}, the maximal number of test sequences is 
$n^{2} \times k^{m-n+1}$ and the maximal length is 
$n^{2} \times m \times k^{m-n+1}$.
In which n is the number of states in the provided automaton, m is the number
of states in the correct automaton, and k is the size of the input alphabet.

By refining the test suites and the specification at the same time, and
using a divide and conquer approach the size of the test is
considerably reduced in comparison to defining the tests from
the final specification\cite{Ipate07}.
This is especially beneficial because complex systems are usually
created in multiple iterations.
Unlike some other test selection methods the W-method does not require
that the number of states is exactly the same between
the implementation and protocol.
The W-method generates tests that guarantee the correct the
correctness of the implementation with a number of states below a
certain upper bound.

The W-method generates input sequences that reach every state in the
diagram, check all the transitions in the diagram and identify all the
destination states and verify them against their counterparts in
the implementation\cite{Ipate07}.
The W-method constructs two sets of input sequences.
A state cover $S \subseteq \Sigma^{*}$ that reaches every state in the
finale state machine.
The empty sequence is also included to reach the initial state.
And a characterization set $W \subseteq \Sigma^{*}$ that has different
outputs for at least one sequence in $W$ for every pair of different states.

We have a specification P that can be modelled by an unknown finite
state machine M.
The only information we need is an estimate of the maximum number of
states of an unknown model for the machine.
Suppose $d = \text{estimated maximum number of states for P}
- \text{number of states in M}$.
Naturally, it follows that $d \geq 0$.
We also take $\Sigma \left[ n \right] = \Sigma^{n} \cup \dots \cup \lbrace \epsilon \rbrace$,
making $\Sigma\left[ n \right]$ a random word with a maximal length of n.
For the W-method the testing suite $T = S \concat \Sigma\left[ d + 1 \right]  \concat W$.
The idea behind the W-method is that $S \concat \Sigma\left[ 1 \right]  = S \cup S
\concat \Sigma$, which is also called the transition cover of $M$, ensures that all the
states and transition in $P$ are also present in $M$, while
$\Sigma\left[ n \right] \concat W$ ensures that $M$ and $P$ are in the same state after
performing all the transitions.

In order to use the W-method there are four requirements.
The machine must be minimal, completely specified, completely reachable, 
and have a fixed initial state.
With some additional manipulation these four assumptions may be violated.
The method consists of three steps.
\begin{enumerate}
\item estimation of the maximum number of states.
\item generate test sequences.
\item verify the responses.
\end{enumerate}

In some fields the W-method can be adapted to reduce the number of test
cases even further such as for server testing.
When a server closes the connection there usually are no further
transitions possible.
This can greatly reduce the number of tests necessary\cite{deRuiter15}.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
