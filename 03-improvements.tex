\documentclass[multi,crop=false,class=article]{standalone}
\onlyifstandalone{\input{common}}

\begin{document}
\section{Improvements}
\label{sec:improvements}
Since Angluin's original paper, several improvements have been made with respect
to the original $L^*$ algorithm described in \cref{sec:an-algor-learn}. The
first improvement that will be discussed is the classification tree. This
improvement forms the basis for multiple subsequent improvements (including the
TTT-algorithm).

In discussing the improvements, two kinds of comparisons are made between them
and the original $L^*$ algorithm. The first comparison is the difference in
query complexity, the second is in terms of the space complexity.

\subsection{Classification Trees}
\label{sec:classification-trees}
Introduced by Kearns and Vazirani\cite{Kearns1994}, classification trees (or
\textit{discrimination trees}) are meant as a replacement for the observation
table (see \cref{sec:data-structure}). In other words, it is a different data
structure for maintaining the necessary information to form a hypothesis
automaton.

The structure of a classification tree is as follows. All internal nodes of the
tree are labelled by a distinguishing extension $e \in E$, such that all the
leaf nodes in its left subtree contains all access strings $s \in S$ such that
$s \concat e \not\in \lang_M$ and all of the leafs in the right subtree contains
those $s$ such that $s \concat e \in \lang_M$. The root of the tree is labelled
with $\epsilon$.

From the way the tree is constructed, any pair of access strings $v,w \in S$ are
distinguished by their \textit{lowest common ancestor} in the tree. It follows
that, since any pair is distinguished by a distinguishing extension, any pair of
access strings in the classification tree are Nerode-inequivalent (see
\cref{def:nerode-eq}). Therefore each access string $s \in S$ uniquely
identifies an equivalence class $\eqclass{s}{\lang_M}$ of the target automaton
$M$.

An essential concept is the \textit{sifting} operation, which for any string $w$
finds the access string representing the equivalence class
$\eqclass{w}{\lang_M}$. The operation works as follows: starting at the root
node, for each internal node labeled with $e$, perform a membership query
$s \concat e$. If the result is \textit{yes}, do the same for the node at the
right tree; if it is \textit{no}, go to the left tree. This is done until a leaf
node has been reached, which is labelled with the correct access string. The
sifting operation can be seen as the replacement for the $\row(w)$ operation for
observation tables, to identify equivalence classes (and thus states) in the
hypothesis. Hypothesis construction is thus done in the same way, using the
sifting operation.

Howar~et~al. state that a classification tree is \textit{redundancy-free}
because each state in $\hat M$ is distinguished by one distinguishing extension,
as opposed to observation tables where states are distinguished by a fixed
amount of distinguishing extensions\cite{Howar2014}.

\subsubsection{Complexity analysis}
\label{sec:complexity-analysis}
With their introduction of the \textit{classification tree}, Kearns and Vazirani
also adapted the original $L^*$ learning algorithm to use the tree
structure\cite{Kearns1994}. The precise operation of the adapted algorithm will
not be given, but we do give an overview of the impact on the query complexity
and the space complexity.

\paragraph{Query Complexity} The amount of \textit{membership queries} for
hypothesis construction is bounded by $\mathcal{O}(kn^2)$ membership queries in
the worst-case (i.e. a degenerated
tree)\cite{Howar2014,Kearns1994,Isberner2014b}. For the counterexample analysis
the amount of queries is bounded by $\mathcal{O}(nm)$\cite{Kearns1994}. Thus in
total the query complexity is bounded by $\mathcal{O}(kn^2 + nm)$, an
improvement over that of $L^*$.

\paragraph{Space Complexity} In chapter 4 of his dissertation, Isberner analyzed
the space complexity of classification tree based learners as follows: a
degenerated tree has $\mathcal{O}(n)$ amount of nodes, of which each inner node
is labeled by a distinguishing extension of length
$\mathcal{O}(m)$\cite{Isberner2015a}.  The space required for the hypothesis
automaton is $\mathcal{O}(kn)$\cite{Isberner2015a}. Thus the algorithm requires
space in $\mathcal{O}(kn + mn)$\cite{Isberner2014b,Isberner2015a}, which is an
improvement on the space complexity of the observation table of $L^*$.

\subsection{Rivest and Schapire's Counterexample Analysis}
\label{sec:rivest-schap-count}
In 1993, Rivest and Schapire published a paper on learning without a reset using
homing sequences\cite{Rivest1993}, which will be the scope of discussion of
\cref{sec:noreset}.

Their paper, however, also included an improvement upon $L^*$, also in the case
that a reset \textit{is} available. Specifically, they improved the
\textit{counterexample analysis} stage, which is the part that handles
counterexamples returned by an equivalence query.

When a counterexample $\gamma$ is obtained, Rivest and Schapire's improved
algorithm finds a \textit{single} extension $e$ from $\gamma$ distinguishing two
states in the new hypothesis and adds it to the set $E$ of distinguishing
extensions. This is in contrast to the original $L^*$ algorithm, which adds all
prefixes of the counterexample to the set $S$ of access strings.

According to Rivest and Schapire\cite{Rivest1993} and further clarified by
Steffen\cite{Steffen2011} and Isberner\cite{Isberner2014a}, there must be a
point where the hypothesis automaton $\hat M$ takes a wrong turn (i.e. there's
an incorrect transition). Let $e$ be the suffix of the counterexample $\gamma$
from the point where the transition of $\hat M$ is inconsistent with the
corresponding transition of the target $M$. Then $e$ can be found by means of a
search strategy such as binary search and membership queries.

As in the previous section, this section concludes with an analysis of the
impact on the amount of membership queries needed and the space requirement.

\paragraph{Query complexity} Rivest and Schapire's original improvement used a
binary search strategy to find the single suffix $e$. As expected from a binary
search, this results in $\mathcal{O}(\log(m))$ membership queries per
counterexample of length $m$. The amount of equivalence queries is
$\mathcal{O}(n)$, thus the amount of membership queries due to
counterexample analysis is $\mathcal{O}(n\log(m))$. The total amount of
membership queries including hypothesis construction is thus
$\mathcal{O}(kn^2 + n\log(m))$, an improvement over that of Kearns and
Vazarani's (see \cref{sec:complexity-analysis}).

It is worthwhile to note that Isberner~et~al. analyzed and compared multiple
search strategies by means of an abstract framework\cite{Isberner2014a}. Their
results show that the query complexities of the other search strategy remain the
same as the original binary search strategy due to Rivest and Schapire, yet
perform better when put into practice.

\paragraph{Space Complexity} From Isberner's analysis\cite{Isberner2014b}, the
space complexity due to the new counterexample analysis is better than that of
$L^*$, however it is worse than that of Kearns and Vazirani's:
$\mathcal{O}(kn^2 + nm)$.

In the next section we will show another version of the algorithm due to
Isberner~et~al., which combines Kearns and Vazirani's classification tree, and
Rivest and Schapire-style counterexample analysis.  \todo{actually write this
  section?}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
