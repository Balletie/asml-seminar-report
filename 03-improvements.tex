\documentclass[multi,crop=false,class=article]{standalone}
\onlyifstandalone{\input{common}}

\begin{document}
\section{Improvements}
\label{sec:improvements}
Since Angluin's original paper, several improvements have been made with respect
to the original $L^*$ algorithm described in \cref{sec:an-algor-learn}. In this
section, the most important improvements are discussed.

In discussing the improvements, two kinds of comparisons are made between them
and the original $L^*$ algorithm. The first comparison is the difference in
query complexity, the second is in terms of the space complexity.

\subsection{Classification Trees}
\label{sec:classification-trees}
Introduced by Kearns and Vazirani\cite{Kearns1994}, classification trees (or
\textit{discrimination trees}) are meant as a replacement for the observation
table (see \cref{sec:data-structure}). In other words, it is a different data
structure for maintaining the necessary information to form a hypothesis
automaton.

The structure of a classification tree is as follows: all internal nodes of the
tree are labeled by a distinguishing extension $e$ and the leaf nodes are
labeled by an access string $s$. For each internal node labeled $e$, the leaf
nodes are in its right or left subtree depending on whether $\mq(s\concat e)$
equals ``yes'' or ``no'' respectively. The root of the tree is labeled with
$\epsilon$.

From the way the tree is constructed, any pair of access strings $v,w \in S$ are
distinguished by their lowest common ancestor in the tree. It follows that,
since any pair is distinguished by a distinguishing extension, any pair of
access strings in the classification tree are Nerode-inequivalent (see
\cref{def:nerode-eq}). Therefore each access string $s \in S$ uniquely
identifies an equivalence class $\eqclass{s}{\lang_M}$ of the target automaton
$M$.

An essential concept is the \textit{sifting} operation (see \cref{alg:sift}),
which for any string $w$ finds the identifying access string of
$\eqclass{w}{\lang_M}$. The operation is denoted as $\sift(w)$ and works as
follows: starting at the root node, for each internal node labeled with $e$, go
left or right in the tree depending on $\mq(s \concat e)$. This is done until a
leaf node has been reached, which is labeled with the correct access string.

The purpose of the $\sift$ operation is the same as the $\row$ operation for
observation tables, namely to identify equivalence classes (and thus states) in
the hypothesis~\cite{Kearns1994}. The construction of the hypothesis $\hat M$ is
thus done in the same way, except using the $\sift$ operation. Also note that
all the leaf nodes that are in the right subtree of the root node define the
accepting states of $\hat M$.

\begin{algorithm}[b]
  \caption{Find the identifying access string of the equivalence class of a
    given word $w \in \Sigma^*$.}
  \label{alg:sift}
  \begin{algorithmic}[1]
    \Function{Sift}{n,w} \Comment{In the first call, $n$ is equal to the root
      node labeled $\epsilon$.}
      \If{$n$ is a leaf node}
        \State\Return $n.\text{label}$
        \Comment{$n.\text{label}$ is the access string that identifies   $\eqclass{w}{\lang_M}$.}
      \EndIf
      \If{$\mq(w \concat n.\text{label})$}
        \Comment{$n.\text{label}$ is a distinguishing extension.}
        \State $\sift(n.\text{right},w)$
      \Else
        \State $\sift(n.\text{left},w)$
      \EndIf
    \EndFunction{}
  \end{algorithmic}
\end{algorithm}

Howar~et~al. state that a classification tree is redundancy-free because each
state in $\hat M$ is distinguished by one distinguishing extension, as opposed
to observation tables where states are distinguished by a fixed amount of
distinguishing extensions\cite{Howar2014}.

\subsubsection{Complexity analysis}
\label{sec:complexity-analysis}
With their introduction of the classification tree, Kearns and Vazirani also
adapted the original $L^*$ learning algorithm to use the tree
structure\cite{Kearns1994}. The precise operation of the adapted algorithm will
not be explained. Nevertheless an overview of the impact on the query- and space
complexity is given below.

\paragraph{Query Complexity} The amount of membership queries for hypothesis
construction is bounded by $\mathcal{O}(kn^2)$ membership queries in the worst
case (i.e. a degenerated tree)\cite{Howar2014,Kearns1994,Isberner2014b}. For the
counterexample analysis the amount of queries is bounded by $\mathcal{O}(nm)$,
which is done $\mathcal{O}(n)$ amount of times over all main loop
executions\cite{Kearns1994}. Thus in total the query complexity is bounded by
$\mathcal{O}(kn^2 + n^2m)$~\cite{Isberner2015a}, an improvement over that of
$L^*$.

\paragraph{Space Complexity} In chapter four of his dissertation, Isberner
analyzed the space complexity of classification tree based learners as follows:
a degenerated tree has $\mathcal{O}(n)$ amount of nodes, of which each inner
node is labeled by a distinguishing extension of length
$\mathcal{O}(m)$\cite{Isberner2015a}. The space required for the hypothesis
automaton is $\mathcal{O}(kn)$\cite{Isberner2015a}. Thus the algorithm requires
space in $\mathcal{O}(kn + mn)$\cite{Isberner2014b,Isberner2015a}, which is an
improvement on the space complexity of the observation table of $L^*$.

\subsection{Rivest and Schapire's Counterexample Analysis}
\label{sec:rivest-schap-count}
Rivest and Schapire made a significant improvement\cite{Rivest1993} to the
\textit{counterexample analysis} stage, which is the part that handles
counterexamples returned by an equivalence query.

When a counterexample $\gamma$ is obtained, Rivest and Schapire's improved
algorithm finds a \textit{single} extension $e$ from $\gamma$ that distinguishes
two states in the new hypothesis and adds it to the set $E$ of distinguishing
extensions. This is in contrast to the original $L^*$ algorithm, which adds all
prefixes of the counterexample to the set $S$ of access strings. Their algorithm
maintains that each row in the observation table is
distinct~\cite{Rivest1993}. Thus, each access string $s \in S$ uniquely
identifies a state\footnote{From this is it also follows that the consistence
  property of the observation table is trivially satisfied~\cite{Rivest1993},
  thereby eliminating the need for consistency checking~\cite{Howar2014}.}, like
what is the case with the classification tree.  However, there is no notion of a
$\sift$ operation in this context, therefore the notation $\access{\hat M}{w}$
is used to denote the unique access string corresponding to a string
$w \in \Sigma^*$.

According to Rivest and Schapire\cite{Rivest1993} and further clarified by
Steffen\cite{Steffen2011} and Isberner\cite{Isberner2014a}, the counterexample
$\gamma$ can be decomposed to find the distinguishing extension: let
$a, e \in \Sigma^*$, $\sigma \in \Sigma$ such that
$\gamma = a \concat \sigma \concat e$ and
$\mq(\access{\hat M}{a}\concat\sigma\concat e) \neq \mq(\access{\hat
  M}{a\concat\sigma}\concat e)$. That is, $e$ witnesses the fact that the states
reached by $\access{\hat M}{a\concat\sigma}$ and
$\access{\hat M}{a}\concat\sigma$ are in fact different, whereas they reach the
same state in $\hat M$ (since $\gamma$ is a counterexample). This means that the
$\sigma$-transition from the state $\access{\hat M}{a}$ is an incorrect
transition. Due to $e$ the states are from that point on correctly
distinguished.

The correct decomposition of $\gamma$ can be found using a search strategy. The
search strategy originally used by Rivest and Schapire is a binary search: first
a membership query is performed with the decomposition in the middle. If it is
accepted, then this means that the next search should be done in the first half
of $\gamma$ (assuming that $\gamma \in \lang_M$, but
$\gamma \not\in \lang_{\hat M}$). Otherwise, the search should be continued in
the second half.

\paragraph{Query complexity} As expected from a binary search, this results in
$\mathcal{O}(\log(m))$ membership queries per counterexample of length $m$. The
amount of equivalence queries is $\mathcal{O}(n)$, thus the amount of membership
queries due to counterexample analysis is $\mathcal{O}(n\log(m))$. The total
amount of membership queries including hypothesis construction is thus
$\mathcal{O}(kn^2 + n\log(m))$, an improvement over that of Kearns and
Vazarani's (see \cref{sec:complexity-analysis}).

It is worthwhile to note that Isberner~et~al. analyzed and compared other search
strategies by means of an abstract framework for counterexample
analysis\cite{Isberner2014a}. Their results show that the query complexities of
the other search strategies remain the same as the original binary search
strategy due to Rivest and Schapire, yet perform better when put into practice.

\paragraph{Space Complexity} From Isberner's analysis\cite{Isberner2014b}, the
space complexity due to the new counterexample analysis is better than that of
$L^*$, however it is worse than that of Kearns and Vazirani's:
$\mathcal{O}(kn^2 + nm)$.

\subsection{The Observation Pack and TTT Algorithms}
\label{sec:ttt}
A rather logical improvement would be to use both the classification tree and
the improved counterexample analysis: the improved algorithm would benefit from
both the improved space complexity of the classification tree and the improved
query complexity of Rivest and Schapire's counterexample analysis. This is the
Observation Pack algorithm due to Howar in a
nutshell~\cite{Howar2012a,Isberner2015a}.

The interaction of Rivest and Schapire's counterexample analysis with the
classification tree is as follows: when the decomposition
$\gamma = a \concat \sigma \concat e$ has been found from the counterexample
$\gamma$ as per \cref{sec:rivest-schap-count}, the string $e$ forms a
distinguishing extension for the states in $\hat M$ represented by
$\access{M}{a\concat\sigma}$ and $\access{M}{a}\concat\sigma$. Thus, the leaf
node corresponding to $\access{M}{a\concat\sigma}$ needs to be split into an
inner node labeled with $e$, where its leaves are labeled by
$\access{M}{a\concat\sigma}$ and
$\access{M}{a}\concat\sigma$~\cite{Isberner2015a}.

The TTT algorithm due to Isberner~\cite{Isberner2014b} builds further upon the
Observation Pack algorithm by adding \textit{discriminator
  finalization}\footnote{Here \textit{discriminator} is a synonym for
  distinguishing extension}. This algorithm considers that the counterexamples
returned from an equivalence query need not be short, in fact they can contain
large amounts of redundancy. The consequence of this is that the space required
for the classification tree can grow significantly and that membership queries
are performed with exceptionally long query parameters, resulting in a slow down
when the algorithm is put into practice.

Therefore the TTT algorithm makes a special effort to reduce these problems by
cleaning up the data structure. When a distinguishing extension $e$ is added to
the classification tree, the TTT algorithm marks it as \textit{temporary}. The
temporary distinguishing extensions are then later replaced with new, typically
shorter, distinguishing extensions, rendering them \textit{finalized}.

\paragraph{Query Complexity} As noted by Isberner~et~al., the query complexity
of both the Observation Pack algorithm and the TTT algorithm coincides with that
of Rivest and Schapire's algorithm; namely $\mathcal{O}(kn^2 + n
\log(m))$. However, as noted in the original publication of the TTT
algorithm~\cite{Isberner2014b}, this is a pessimistic query complexity as in
practice there is a significant difference. In the same publication it is
concluded that this difference is due to the fact that discriminator
finalization results in trees that are more balanced. Thus the $\sift$ operation
requires a smaller amount of membership queries, leaning more towards
$\mathcal{O}(\log(n))$ for a perfectly balanced tree instead of $\mathcal{O}(n)$
for a degenerated tree.

\paragraph{Space Complexity} As it makes no effort to improve the space
requirements, the space complexity of the Observation Pack algorithm remains the
same as Kearns and Vazirani's algorithm (\cref{sec:complexity-analysis}):
$\mathcal{O}(kn + nm)$.

The TTT algorithm, in contrast, has an optimal space complexity of $\Theta(kn)$,
due to the fact that the space needed for storing the hypothesis dominates the
required space~\cite{Isberner2014b,Isberner2015a}.
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
