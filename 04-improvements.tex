\documentclass[multi,crop=false,class=article]{standalone}
\onlyifstandalone{\input{common}}

\begin{document}
\section{Improvements}
\label{sec:improvements}
There are several improvements to be made with respect to the algorithms
described in \cref{sec:variants,sec:fundamental-theory}, using different data
structures which decrease the space complexity.

\subsection{Classification Trees}
\label{sec:classification-trees}
Introduced by Kearns and Vazirani\cite{Kearns94}, classification trees (or
\textit{discrimination trees}) are meant as a replacement for the observation
table (see \cref{sec:data-structure}). In other words, it is a different data
structure for storing the sets $S$ and $E$ of access strings and distinguishing
extensions. The main characteristic of a classification tree is that it is
\textit{redundancy-free}, which will be explained in this section.

Say that we have a classification tree for some target automaton $M$. The labels
of the internal nodes of the tree are distinguishing extensions from the set
$E$, and the labels of leaf nodes are access strings from $S$. The tree's
structure is such that for any internal node $e \in E$, its left subtree
contains all $s \in S$ such that $s \concat e$ is rejected by $M$, and its right
subtree contains those $s$ such that $s \concat e$ is accepted by $M$.

From the way the tree is constructed, any pair of access strings $v,w \in S$ are
distinguished by their \textit{lowest common ancestor} in the tree. Thus, $v$
and $w$ are distinguished by exactly \textit{one} distinguishing extension. In
other words, any pair of access strings are Nirode-inequivalent~\todo{reference
  subsection chpt 2}, and thus each access string uniquely identifies a distinct
equivalence class of the target automaton $M$. When we construct a hypothesis,
each equivalence class (and thus each access string), corresponds to a state in
our hypothesis automaton $\hat M$.

Howar~et~al. note that this means that a discrimination tree is
\textit{redundancy-free}, as opposed to observation tables where states are
distinguished by a fixed amount of distinguishing extensions\todo{Verify
  this. What do Howar et al mean with ``fixed number of
  discriminators''?}\cite{Howar14}.

\subsubsection{Constructing a Hypothesis}
\label{sec:constr-hypoth}
To construct the hypothesis automaton $\hat M$ from the classification tree $T$,
the algorithm creates a state for each leaf (access string) of $T$. Then, to
create the transitions for each state identified by some access string $s$, the
algorithm has to find the access string uniquely identifying the state which is
reached by executing $\hat M$ with input $s \concat b$, where $b \in
\Sigma$. This is done by traversing the tree using membership queries: starting
at the root node, for each internal node labeled with $e$, perform a membership
query $s \concat b \concat e$. If the result is \textit{yes}, do the same for
the node at the right tree; if it is \textit{no}, go to the left tree. When a
leaf has been reached, the algorithm has found the access string which uniquely
identifies a state in $\hat M$. Kearns and Vazirani call this operation
\textit{sifting}, and is denoted as $\textbf{Sift}(s, T)$\cite{Kearns94}.

So the state identified by $t$ reached by a $b$-transition from another state
identified by $s$, is found by executing the operation
$\textbf{Sift}(s \concat b,T)$. The algorithm then creates a $b$-transition from
the state identified by $s$ to the state identified by $t$. It does this for
each $s \in S$, for each $b \in \Sigma$, after which the construction of
$\hat M$ is finished. The hypothesis can then be used to perform an equivalence
query, resulting in a counterexample or the answer \textit{yes}, in which case
the algorithm is done learning.
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End: